# DDP 多卡训练实现说明

## 📋 实现方案

**标准数据并行（Standard Data Parallel）**

### 核心原理

```
单卡训练（batch_size=64）：
┌─────────────────────┐
│ 采样 64 个样本      │
│ 计算梯度: g         │
│ 更新: θ -= lr * g   │
└─────────────────────┘

8卡分布式（每卡batch_size=64）：
┌──────────┐  ┌──────────┐  ...  ┌──────────┐
│ 采样 64  │  │ 采样 64  │       │ 采样 64  │
│ 计算 g0  │  │ 计算 g1  │  ...  │ 计算 g7  │
└────┬─────┘  └────┬─────┘       └────┬─────┘
     │             │                   │
     └─────────────┼───────────────────┘
                   ▼
            AllReduce(sum)
           (g0 + g1 + ... + g7)
                   ▼
              平均梯度
          (g0 + ... + g7) / 8
                   ▼
    更新: θ -= lr * avg_gradient

结果：有效 batch_size = 64 × 8 = 512
```

## 🎯 关键特性

| 项目 | 值 |
|-----|-----|
| **每卡采样** | batch_size = 64 |
| **有效 batch_size** | 64 × 8 = 512 |
| **梯度同步** | DDP 自动平均（AllReduce） |
| **学习率** | 保持不变（0.0001） |
| **模型初始化** | 所有卡一致（相同 seed） |
| **数据采样** | 每卡不同（seed + rank） |
| **总计算量** | 约等于单卡 batch_size=512 |

## ✅ 实现细节

### 1. 模型初始化（一致性）

```python
# 第一步：相同 seed，所有 GPU 模型权重一致
torch.manual_seed(args.seed)

model = DDPG_DDP(...)  # 所有 GPU 初始化相同
```

### 2. 数据采样（多样性）

```python
# 第二步：不同 seed，每 GPU 采样不同数据
torch.manual_seed(args.seed + rank)

# GPU0: seed=1, GPU1: seed=2, ..., GPU7: seed=8
# 导致采样的数据完全不同
```

### 3. 梯度同步

```python
# DDP 在 backward() 时自动执行：
# 1. AllReduce(sum)：收集所有 GPU 的梯度
# 2. 除以 world_size：平均梯度
# 3. 每个 GPU 收到相同的平均梯度

model.update()  # DDP 自动处理梯度同步
```

### 4. 优化器更新

```python
# 所有 GPU 在相同的平均梯度上进行相同的更新
optimizer.step()

# 结果：所有 GPU 的模型权重保持一致
```

## 📊 性能分析

### 理论加速比

- **梯度计算**：8 倍并行（8 个 GPU 各计算梯度）
- **实际加速**：~7-7.5 倍（受通信开销影响）
- **环境采样**：无加速（CPU 串行，无法并行）

### 实际场景

如果单卡训练需要 100 小时，8 卡应该需要：
- 理论：100 / 8 = 12.5 小时
- 实际：100 / 7 ≈ 14-15 小时（考虑通信和环境采样开销）

## 🚀 使用方式

### 基础启动

```bash
# 自动检测 GPU 数量
chmod +x train_ddp.sh
./train_ddp.sh

# 或直接用 torchrun
torchrun --nproc_per_node=8 rl_trainer/main_ddp.py
```

### 自定义参数

```bash
# 增加 episodes（更多计算）
torchrun --nproc_per_node=8 rl_trainer/main_ddp.py --max_episodes 12500

# 增加 batch_size（每卡采样更多数据）
torchrun --nproc_per_node=8 rl_trainer/main_ddp.py --batch_size 128

# 组合
torchrun --nproc_per_node=8 rl_trainer/main_ddp.py \
  --max_episodes 12500 \
  --batch_size 128
```

## 🔍 正确性检查

✅ **模型权重一致性**
- 所有 GPU 初始权重相同：相同 seed
- 训练后权重相同：DDP 梯度同步

✅ **梯度同步正确性**
- DDP 自动平均梯度
- 每个 GPU 收到相同的平均梯度
- 优化器更新结果一致

✅ **数据多样性**
- 每卡采样不同数据（seed + rank）
- 高效利用多 GPU 的计算能力

✅ **计算量一致**
- 单卡 50k episodes：梯度更新 50000 次
- 8 卡 6250 episodes：梯度更新 6250 × 8 = 50000 次

## 📝 与原始代码的差异

| 方面 | 单卡版本 | DDP 版本 |
|------|---------|---------|
| **batch_size** | 64 | 每卡 64 |
| **有效 batch** | 64 | 512 |
| **episodes** | 50000 | 6250 |
| **梯度同步** | 无 | DDP 自动平均 |
| **学习率** | 0.0001 | 0.0001（保持） |
| **预期训练时间** | ~100h | ~14-15h |

## 🎓 理论背景

这是标准的 **数据并行（Data Parallelism）** 实现：

1. **数据划分**：将 batch 分成 8 份，每个 GPU 处理一份
2. **本地计算**：每个 GPU 计算本地的梯度
3. **梯度聚合**：AllReduce 收集并平均所有梯度
4. **权重更新**：所有 GPU 使用平均梯度更新

这是分布式训练的最基础、最稳定的方案。

## ⚠️ 注意事项

1. **通信开销**：DDP 梯度同步需要网卡通信，可能成为瓶颈
2. **内存占用**：每个 GPU 需要完整的模型权重副本
3. **环境采样**：游戏环境采样是 CPU 操作，无法并行，可能限制加速比
4. **batch_size 影响**：有效 batch_size=512 可能影响收敛性，可根据需要调整

## ✨ 总结

当前实现是**合理的标准数据并行**：
- ✅ 正确的梯度同步
- ✅ 高效的 GPU 利用
- ✅ 总计算量与单卡相同
- ✅ 简单且稳定
